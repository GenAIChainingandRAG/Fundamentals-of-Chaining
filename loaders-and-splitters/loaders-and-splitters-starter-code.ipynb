{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631e33ca",
   "metadata": {},
   "source": [
    "![Document Loaders and Splitters](../assets/document-loaders-and-splitters.png)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e2d22",
   "metadata": {},
   "source": [
    "### Learning objective:\n",
    "By the end of this lesson, you will be able to manage documents with document loaders and splitters. \n",
    "\n",
    "\n",
    "### About:  \n",
    "Often, we need to format documents before we can use the prompts. In this lesson, we will use loaders and splitters to format documents from file directories, csv files, and websites. \n",
    "\n",
    "\n",
    "### Prerequisites:\n",
    "- Python (required) \n",
    "- Intro to LangChain and prior prompt  eng. lessons (required) \n",
    "- Visual Studio Code (recommended)\n",
    "- GitHub Copilot lessons (recommended) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbfb2a1",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. [Imports](#imports)\n",
    "1. [Document Loaders](#loaders)\n",
    "1. [Text splitters](#splitters)\n",
    "\n",
    "### Activities\n",
    "1. [Lab](#lab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeddef4",
   "metadata": {},
   "source": [
    "## Installs\n",
    "\n",
    "You may need to install the following tools\n",
    "\n",
    "\n",
    "- %pip install --upgrade  tiktoken\n",
    "- %pip install --upgrade  \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc10da",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68385797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI #openai chatbot\n",
    "from langchain_core.prompts import ChatPromptTemplate #template for chat prompts\n",
    "from langchain_core.output_parsers import StrOutputParser #output parser for string output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2c174",
   "metadata": {},
   "source": [
    "<a id='loaders'></a>\n",
    "## Document Loaders\n",
    "Document loaders allow you to bring in text from another source, such as a file on your computer, data from a website, or even YouTube video transcripts. \n",
    "LangChain offers functionality to help load and store those documents in so that they can be used by the language model. \n",
    "\n",
    "[LangChain Docs](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "\n",
    "### Supported loaders:\n",
    "- CSV\n",
    "- File Directory\n",
    "- HTML\n",
    "- JSON\n",
    "- Markdown\n",
    "- PDF\n",
    "\n",
    "\n",
    "Additionally, LangChain allows offers multiple 3rd party integrations to help with loading documents of different formats and from common sites (like YouTube, and Hackernews). \n",
    "\n",
    "[Integrations](https://python.langchain.com/docs/integrations/document_loaders/)\n",
    "\n",
    "\n",
    "We will work with several documents and load them in for this lab. \n",
    "\n",
    "#### Data sources stored in assets folder:\n",
    "- **CSV Source:** Fanaee-T, Hadi, and Gama, Joao, \"Event labeling combining ensemble detectors and background knowledge\", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3.\n",
    "- **Webloader Source:** [GPT-4 article](https://arxiv.org/html/2303.08774v4)\n",
    "- **Markdown Source:** [OpenAI Github README.md example](https://github.com/openai/openai-cookbook?tab=readme-ov-file)\n",
    "- **File Directory Source:** Sample Meeting text files generated by GPT and stored in \"my_docs\" folder\n",
    "\n",
    "In each example below you will see the import, the loader, and print statement to view the doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Directory\n",
    "## Note: You'll see this code again in the summarization lab! \n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader('assets/my_docs')\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb615afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebBaseLoader \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://arxiv.org/html/2303.08774v4\")\n",
    "docs = loader.load()\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown File \n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "markdown_path = \"assets/markdown_example.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c0067",
   "metadata": {},
   "source": [
    "<a id='splitters'></a>\n",
    "## Text Splitters\n",
    "\n",
    "Large documents (or collections of documents) often need to be split or chunked in such a way that it is more meaningful for application and manageable for the language model you are using. **Reminder:** language models limit the size of what you can pass to and get back form them, e.g., GPT-4 Turbo has a context window of 128k tokens which is for both the prompt and response.\n",
    "\n",
    "[docs](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "\n",
    "\n",
    "### How text splitters work\n",
    "- Split into small, semantically meaningful pieces. Sentences are common. \n",
    "- Combine those into meaningful chunks as defined by a function of your choice with a defined size.\n",
    "- Create a new “document” from that chunk and continue through the text. Each chunk will have overlap with the previous chunk and the following chunk. \n",
    "\n",
    "### You control\n",
    "- What text is split on \n",
    "- How chunks are created \n",
    "\n",
    "### LangChain Supported splitters\n",
    "- [docs](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- We will focus on the [recursive splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter) which is recommended for generic text for this lab\n",
    "- We will use a [token splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6bc7f",
   "metadata": {},
   "source": [
    "### Recursive splitter recommend for generic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c65ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nursery_rhyme = f\"\"\"\n",
    "Hey, diddle, diddle,\n",
    "The cat and the fiddle,\n",
    "The cow jumped over the moon;\n",
    "The little dog laughed\n",
    "To see such sport,\n",
    "And the dish ran away with the spoon.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a15956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive splitter recommend for generic text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=75,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5267690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine split documents\n",
    "texts = text_splitter.create_documents([nursery_rhyme])\n",
    "print(texts[0])\n",
    "print(texts[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.split_text(nursery_rhyme)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fc932",
   "metadata": {},
   "source": [
    "### Token splitter \n",
    "Language models have a token limits so it can be helpful to split on tokens when planning to pass docs to a LLM. [docs](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)\n",
    "\n",
    "Note: This code is also used in the summarization lab from the Advanced Prompting module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Text Splitter Example (simple splitter )\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "# print(split_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c74192",
   "metadata": {},
   "source": [
    "<a id='lab'></a>\n",
    "## Lab\n",
    "\n",
    "1. Load my_documents from file folder and split with a token splitter \n",
    "2. Load a document from wikipedia \"https://en.wikipedia.org/wiki/Ancient_Rome\" and split with recursive text splitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ef254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b splitter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a web loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83975f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b splitter \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
