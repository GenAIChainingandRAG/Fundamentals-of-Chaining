{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51634aa0",
   "metadata": {},
   "source": [
    "![Introduction to LangChain](../assets/introduction-to-langchain.png)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff2e374",
   "metadata": {},
   "source": [
    "### Learning objective:\n",
    "By the end of this lesson, students will be able to import and call a chat model in LangChain and create a basic chain using prompt templates. \n",
    "\n",
    "### About:  \n",
    "This is an introduction to LangChain and a demo of core functionality. \n",
    "\n",
    "### Prerequisites:\n",
    "- Python (required) \n",
    "- Visual Studio Code (recommended)\n",
    "- GitHub Copilot lessons (recommended) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb7aa5",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. [Importing Open AI and LangChain](#imports)\n",
    "1. [Basic Model calls](#basic)\n",
    "1. [Intro to LangChain](#lang-chain)\n",
    "1. [Basic chain](#basic-chain)\n",
    "1. [Prompt Templates](#prompt-templates)\n",
    "1. [LangChain Expression Language (LCEL)](#lcel)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Activities\n",
    "1. [Try it! 1](#act-1)\n",
    "1. [Try it! 2](#act-2)\n",
    "1. [Try it! 3](#act-3)\n",
    "1. [Lab](#lab)\n",
    "\n",
    "\n",
    "### References\n",
    "1. [LangChain Intro](https://python.langchain.com/docs/expression_language/get_started)\n",
    "1. [LangChain LCEL](https://python.langchain.com/docs/expression_language/)\n",
    "1. [LangChain Interface](https://python.langchain.com/docs/expression_language/interface)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb5694",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be060d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI #openai chatbot\n",
    "from langchain_core.prompts import ChatPromptTemplate #template for chat prompts\n",
    "from langchain_core.output_parsers import StrOutputParser #output parser for string output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eabefc",
   "metadata": {},
   "source": [
    "<a id='basic'></a>\n",
    "## Basic Model Calls \n",
    "Let's dive right in and see the tools in action. Afterward, we will walk through each component to see how it works. \n",
    "\n",
    "**Steps:**\n",
    "1. Specify model (here we will ChatGPT) \n",
    "1. Invoke the model on a prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. specify your model. \n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c40bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. invoke the model on a prompt\n",
    "llm.invoke(\"Where are 2 famous tourist spots?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7532764",
   "metadata": {},
   "source": [
    "<a id='act-1'></a>\n",
    "### Try it! \n",
    "Invoke the model with a prompt of your choice. Share your results with the class. \n",
    "\n",
    "**Sample code:**\n",
    "```python\n",
    "llm.invoke(\"insert your prompt here!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514eee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993292e",
   "metadata": {},
   "source": [
    "<a id='lang-chain'></a>\n",
    "## Intro to LangChain\n",
    "---\n",
    "\n",
    "LangChain makes it possible to create common chains of AI tasks with a lot less code. With chains, code becomes more modular so it is easier to test, swap components (such as models or output formats), edit, and automate. This becomes very important as you work to move your models to production. \n",
    "\n",
    "#### Example chain: Model | Prompt | Output Parser \n",
    "\n",
    "\n",
    "### Key components \n",
    "####  Models- the AI model being used \n",
    "1. **LLMs:** Such as GPT-4 typically take strings\n",
    "1. **ChatModels:** Takes roles and messages they are built on top of LLMs but offer additional functionality.  *Note: We will focus on the Chat Models implementation in our labs* \n",
    "    1. **Chat Model Messages:** Chat messages by different roles \n",
    "            - Basic Roles:\n",
    "                - Human Message\n",
    "                - AI message\n",
    "            - Advanced roles (not supported by all models) \n",
    "                - System Message\n",
    "                - Function Message \n",
    "                - Tool Message\n",
    "                \n",
    "#### Prompts: Inputs to the language model \n",
    "- Often user message is modified in some way before being passed as prompt to the language model \n",
    "- Prompt templates: Take a string, format it in some way, and pass it to the language model \n",
    "\n",
    "#### Output Parsers: Way to format output returned from model\n",
    "- StrOutputParser: Formats messages as strings\n",
    "- Agent Parsers: Learn more in the agent module \n",
    "\n",
    "#### Basic chains\n",
    "- 2 components (simplest  chain): Model | Prompt  \n",
    "- 3 component chain (very common chain): Model | Prompt | Output Parser \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46b7eae",
   "metadata": {},
   "source": [
    "<a id='basic-chain'></a>\n",
    "## Create a basic chain using chat prompt template \n",
    "\n",
    "1. **Create a prompt template.** \n",
    "Let's create a prompt template to generate techical documentation based on user input. Notice that we set the system persona to \"You are world class travel writer\". \n",
    "\n",
    "    ```python \n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are world class travel writer.\"), #system persona is set \n",
    "        (\"user\", \"{input}\") #user input is set, here we will pass it an input variable \n",
    "    ])\n",
    "    ```\n",
    "\n",
    "2. **Specify your chain.**  \n",
    "Here we will create a simple chain with the prompt template and our chat model \n",
    "    ```python\n",
    "    chain = prompt | llm \n",
    "    ```\n",
    "3. Finally you can invoke your chain with a prompt of your choice \n",
    "    ```python\n",
    "       chain.invoke({\"input\": \"Where are 2 famous tourist spots?\"})\n",
    "    ```\n",
    "    \n",
    "    \n",
    "#### Note:\n",
    "There are two common options for your templates. We will experiment with both in this lab. \n",
    "\n",
    "1. ``` ChatPromptTemplate.from_messages()```  is a chat template that includes both a system and user. In these, you can specify the system persona. Play around with these try changing \"You are world class technical documentation writer\" to a \"helpful pirate\". \n",
    "1. ``` ChatPromptTemplate.from_template()``` is simpler and takes only the user input. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9304b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a prompt template \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class travel writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Specify your chain \n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29557ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Finally you can invoke your chain with a prompt of your choice (this will call your Chat Model!)\n",
    "resp= chain.invoke({\"input\": \"Where are 2 famous tourist spots?\"})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14779c",
   "metadata": {},
   "source": [
    "<a id='act-2'></a>\n",
    "### Try it!\n",
    "- Edit the above code to change the system persona from \n",
    "    ```\"You are a world class travel writer.\"```  to ```\"You are a helpful assistant.\"```\n",
    "- Compare the responses from ChatGPT. What changed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e93c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a prompt template \n",
    "# Edit this step! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ba396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Specify your chain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Finally you can invoke your chain with a prompt of your choice (this will call your Chat Model!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255b81c",
   "metadata": {},
   "source": [
    "<a id='prompt-templates'></a>\n",
    "## Prompt Templates \n",
    "As we saw above, prompt templates can be helpful for defining system personas and pasing in prompts to ChatGPT. You can add a lot more flexiblity to these tempates by passing variables in to the template. Earlier we used a single variable ```{input}```.  Let's see an example with multiple varibales. \n",
    "\n",
    "Note: Here we will use the simpler ```ChatPromptTemplate.from_template()``` to focus on our variables, but you can also use this technique with ```ChatPromptTemplate.from_messages```\n",
    "#### Review the code below. \n",
    "```python \n",
    "    prompt_temp = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\")\n",
    "    \n",
    "    prompt_temp.format(adjective=\"funny\", content=\"dog\")\n",
    "```\n",
    "\n",
    "#### Note: \n",
    "Prompts are almost always passed as strings. It may be helpful to review [Python String formatting](https://realpython.com/python-string-formatting/#2-new-style-string-formatting-strformat) and [String Interpolation / f-Strings](https://realpython.com/python-string-formatting/#3-string-interpolation-f-strings-python-36) to help you automate your prompts. \n",
    "\n",
    "#### Reference: \n",
    "Prompts below inspired by [LangChain Docs](https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1727d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your template\n",
    "prompt_temp = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b519569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defne your chain \n",
    "chain2 = prompt_temp | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ae7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call your model with your joke preferences \n",
    "\n",
    "resp= chain2.invoke({\"adjective\": \"funny\", \"content\": \"dog\"})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c7915",
   "metadata": {},
   "source": [
    "<a id='act-3'></a>\n",
    "### Try it! \n",
    "Output 3 new jokes on the topics of your choice! \n",
    "\n",
    "Edit the following for new outputs: \n",
    "```python \n",
    "resp= chain2.invoke({\"adjective\": \"your adjective\", \"content\": \"your topic\"})\n",
    "resp \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Joke 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Joke 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Joke 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5e199",
   "metadata": {},
   "source": [
    "<a id='chains'></a>\n",
    "## Chains with Outputs\n",
    "\n",
    "The most common chain includes a model, a prompt template, and output parser. \n",
    "\n",
    "**Output parsers** are a way to format the response you get from the language model. \n",
    "\n",
    "#### Common Output Options:\n",
    "\n",
    "1. ``` StrOutputParser ``` formats messages as strings  \n",
    "1. ```Agent Parsers``` more to come on this in agent module \n",
    "\n",
    "\n",
    "#### New Steps\n",
    "1. Specify an output parser\n",
    "    ```python \n",
    "        output_parser = StrOutputParser()\n",
    "    ```\n",
    "2. Add your output parser to your chain\n",
    "    ```python \n",
    "        chain = prompt | llm | output_parser\n",
    "    ```\n",
    "\n",
    "Let's add this to our joke chain. Already have our prompt template and model set. Let's specify the output parser and add it to our chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  Specify an output parser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Add your output parser to your chain\n",
    "chain3 = prompt_temp | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Invoke our model and observe the change in the output \n",
    "resp= chain3.invoke({\"adjective\": \"funny\", \"content\": \"dog\"})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071a68a",
   "metadata": {},
   "source": [
    "<a id='lcel'></a>\n",
    "##  LangChain Expression Language (LCEL)\n",
    "**LCEL** allow you build complex chains with runnable components greatly reducing the amount of code you need to write for common model tasks. They come with common methods or interfaces that you can use in many situations. So far we have used invoke. Here are a few others you may encounter: \n",
    "\n",
    "#### Interface\n",
    "- Invoke (invoke)- pass in a string and get back a string\n",
    "- Stream (stream)- streams the response \n",
    "- Batch (batch)- pass in multiple prompts and get back the response \n",
    "- Async (ainvoke) - run asynchronously \n",
    "\n",
    "#### Docs\n",
    "1. [LangChain LCEL](https://python.langchain.com/docs/expression_language/)\n",
    "1. [LangChain Interface](https://python.langchain.com/docs/expression_language/interface)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71515d2",
   "metadata": {},
   "source": [
    "<a id='lab'></a>\n",
    "# Lab\n",
    "#### Objective: \n",
    "Your goal is to create a chain that take a number (n) and destination and outputs the top n attractions in your destination of choice. \n",
    "\n",
    "#### Steps\n",
    "1. Create a prompt template (```ChatPromptTemplate.from_template()```) that takes the following prompt from the user: \n",
    "    ```python \n",
    "     (\"user\", \"List the top {number} attractions in {destination}\") #input with 2 variables  \n",
    "  \n",
    "   ```\n",
    "1. Specify your language model (ok to use the same as before) \n",
    "2. Specify you output parser (ok to use the same as before) \n",
    "1. Build a chain with your prompt template, language model, and output parser \n",
    "1. Invoke your model at least 3 times for different locations and number of attractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create prompt template \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Specify language model (Chat GPT here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31007990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Set your output parser \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb49efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Build your chain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70702b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke chain- 1 try different inputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke chain- 2 try different inputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke chain- 3 try different inputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bdbc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
